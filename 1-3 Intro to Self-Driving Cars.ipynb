{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zsunberg/ai4all-berkeley-driving/blob/master/1-3 Intro to Self-Driving Cars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Intro to Self-Driving Cars\n",
    "\n",
    "Hello! In this notebook we'll explain the problem we'll be tackling during the program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Colab Code to get everything set up\n",
    "! git clone https://github.com/zsunberg/ai4all-berkeley-driving\n",
    "%cd ai4all-berkeley-driving/\n",
    "\n",
    "# Get the Dependencies for the Colab Notebook\n",
    "! pip install stable-baselines\n",
    "! pip install celluloid\n",
    "! pip install numpy==1.17.0\n",
    "\n",
    "# Import the API\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from driving.ui import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State, Actions, and Reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our toy problem, we've tried to simplify the task as much as possible to make training an RL autonomous driving agent to be as straightforward as possible. As disucssed before, we have to specify at least 3 things in order to have a complete RL problem: the state, reward, and possible action. \n",
    "\n",
    "## The State\n",
    "\n",
    "The state itself is the easiest part of the project. For the purposes of our project, our car, Clementine, is represented by an (x,y) pair. Additionally, Clementine can be facing a certain direction, given by an angle (called theta), ranging from 0 to 360 degrees. Below is an image of the track we'll be working with. ![Image of the Track](assets/self_driving_car.png) \n",
    "\n",
    "As we can see, in the above image, Clementine is roughly around the the x coordinate of 1.4, the y coordinate of 0.3, and tilted about at about an angle of 10 degrees. Therefore, we represent the current state of Clementine as the list $(1.4, 0.3, 10)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Action\n",
    "\n",
    "The second part of the RL problem is specifying the possible actions that our action can take. RL problems require trying different actions available in order to understand which actions lead to the best reward. Since training Neural Nets can take a lot of time, we tried to reduce the number of actions that Clementine can take. Because Clementine exists in a simulation and safety isn't a concern, we've made Clementine move at a constant speed throughout the entire simulation.\n",
    "\n",
    "This means that the only thing you have to worry about is getting Clementine to turn to stay on the track! We've given Clementine the ability to take 9 types of turns, all given by the degree of the turn: $(-35, -20, -10, -5, 0, 5, 10, 20, 35)$\n",
    "\n",
    "To give an example, in state (1.5, 0.5, 0), Clementine is on the road facing forward. Since we want Clementine to stay on the road, the best action to take would probably be not turning at all (aka, making a turn of 0 degrees). Likewise, if Clementine started in state (0.25, 1.5, 0), we'd want Clementine to turn to get back on the road, but not too much as to overshoot it. In that case, a turn of 10 or possibly 20 degrees would be best. As we can see from this example, the best action depends heavily upon the state that Clementine is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward\n",
    "\n",
    "In the final part of the RL problem, we specify what we mean by a \"good\" action. In other words, each action Clementine takes gives it a reward that tells it how good that action was. Becase we want Clementine to stay on the road, we want to give Clementine a reward for being close to the road, but then penalize Clementine for being too far away or off the track. \n",
    "\n",
    "Later, we'll have you design your own reward function to keep Clementine on the track, but, for the time being, we're providing a default reward function to let you train an initial model. In the image below, we explain two of the measures we use to calculate the reward:\n",
    "\n",
    "![Image of Reward](assets/part_of_reward_function.png) \n",
    "\n",
    "The first measure we use is the distance between Clementine and the road. Take special note that this is measured by the distance of the state (x,y,theta), which is the center of Clementine, and the point on the road Clementine is closest to.\n",
    "\n",
    "The second measure we use is the angle between where Clementine's going and the direction of the road (where the road is moving counterclockwise). \n",
    "\n",
    "The third measure that's not in the above picture is called the action penalty. The action penalty tries to discourage Clementine from taking too extreme turns, thereby encouraging Clementine to stay on the road, especially when the road is straight. The action penalty will be larger the larger the turn is. \n",
    "\n",
    "The default reward we give you works as follows: \n",
    "\n",
    "\n",
    "1. If Clementine is more than 0.1 away from the road but less than 2 away from the road, give Clementine a reward of -d - a - ap, where d is the distance of Clementine from the road, a is the angle between Clementine and the road, and ap is the action penalty.\n",
    "\n",
    "\n",
    "2. If Clementine is more than 2 away from the road, give Clementine a reward of -20 and stop execution. \n",
    "\n",
    "Now it's important to note that this reward we gave you isn't the only reward that we could use for this problem. We could instead choose to design the reward function differently that prioritizes different parts of the map or actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Policy\n",
    "\n",
    "Now that we have a sense of state, actions, and reward, we need to figure out how to go from a state to an action, with information from the reward. We can think of the policy as a function with the state as a parameter, and it returns the function. To give an example, let's look at the following policy that always returns a single action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Create the Environment Real Quick\n",
    "env = DrivingEnv()\n",
    "actions = env.actions # The actions\n",
    "\n",
    "\n",
    "# This is a great policy, the best policy\n",
    "def super_smart_policy(x,y,theta):\n",
    "    # chosen_action = actions[4] # You can also use the actions\n",
    "    chosen_action = 0 # Always go straight\n",
    "    \n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's Visualize the Above Policy\n",
    "animate(env, super_smart_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the above policy didn't work all that well. Below, try writing a policy that works a bit better and gets the car to stay on the road! \n",
    "\n",
    "Things to consider:\n",
    "1. How can we use the information of the state (x,y,theta) to get the car to stay on the road?\n",
    "2. How can we use the distance/angle to get the car to stay on the road?\n",
    "3. How can we use the reward to help the car stay on the road?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own policy and animate it! \n",
    "# Remember, a policy returns an action, how you choose that action is up to you!\n",
    "# returns: an action\n",
    "env = DrivingEnv()\n",
    "actions = env.actions\n",
    "\n",
    "def policy(x,y,theta):\n",
    "    # If you want to use the reward or the distance/angle, the below code let's you do that!\n",
    "#     d, ang = env.getDistanceAngle(x,y,theta) # This gives you the distance and angle described above\n",
    "#     reward = env.reward(x,y,theta,actions[0]) # This is an example of how to get the reward given (x,y,theta) and (action)\n",
    "\n",
    "    # Student Code Here:\n",
    "    # The Below For-Loop is a Python way to go through the elements of a list, here we're\n",
    "    # going over all the actions.\n",
    "    for action in actions: \n",
    "        pass # Delete me and put code here!\n",
    "\n",
    "    return -35 # Try choosing the action in a smarter way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's Visualize the Above Policy\n",
    "animate(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
