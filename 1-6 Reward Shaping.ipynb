{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "1-6 Reward Shaping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zsunberg/ai4all-berkeley-driving/blob/master/1-6%20Reward%20Shaping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeKPHZSDMc0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/zsunberg/ai4all-berkeley-driving\n",
        "! pip install stable-baselines\n",
        "! pip install celluloid\n",
        "! pip install numpy==1.17.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZMZr4SxM0mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ai4all-berkeley-driving"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0MOAlLeMbpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from driving.ui import *\n",
        "env = DrivingEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATrhA0mgMbpw",
        "colab_type": "text"
      },
      "source": [
        "# 1. Goal\n",
        "\n",
        "Go to the track and stay on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yaPt2tTMbp2",
        "colab_type": "text"
      },
      "source": [
        "# 2. Reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r60y4GD_Mbp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_reward(x, y, theta, a):\n",
        "    # Put your reward function here\n",
        "    # you might want to use the (signed) distance from the track, d\n",
        "    d, angle = env.distance_angle(x, y, theta)\n",
        "    return 0.0\n",
        "    \n",
        "env.reward = my_reward\n",
        "plot_reward(my_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkHb2UalMbp_",
        "colab_type": "text"
      },
      "source": [
        "# 3. DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYX7sYWtMbqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = StudentQModel(env, exploring_rate=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-mXalAHMbqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train(20000, model_name=\"my_model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6TfE4WMbqN",
        "colab_type": "text"
      },
      "source": [
        "# 4. Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiqtzeumMbqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_q_policy(x, y, theta):\n",
        "    # implement a policy that maximizes model.q_value(x, y, theta, a)\n",
        "    # just like you did for the reward function\n",
        "    return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyvpvqu5MbqW",
        "colab_type": "text"
      },
      "source": [
        "# 5. Goal Accomplished!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKXUL7rjMbqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cam = record(env, my_q_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6F-iTwMbqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cam.animate().save(\"my_simulation.mp4\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XujWcCFLNZdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}